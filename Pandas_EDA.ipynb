{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2824917,"sourceType":"datasetVersion","datasetId":1727728},{"sourceId":2965537,"sourceType":"datasetVersion","datasetId":1818188}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A Basic Pandas and EDA Tutorial using the Titanic Dataset\n\n## Your starting step to ML\n\n1. **Pandas - A Data Manipulation library - It will be your handy assistant for anything data in python.**\n2. **EDA - Exploratory Data Analysis - As the name suggests you explore the data at hand to fully comprehend it. Once you thoroughly understand your data, you will know what to do with your model**","metadata":{}},{"cell_type":"markdown","source":"---\n### Pandas \nWe will use this for everything data. \nLoading, Manipulating and Preprocessing.\nNow cleaning your data and processing it will be one of the most tantalizing tasks of your ML career, but its the most rewarding and important, cause data makes or breaks your entire model. You will also grow as a person as data preprocessing will teach you the virtue of patience.\n\n---\n\n### EDA\n\nIt will look like a few colorful charts but, it's more than that, if you don't do this, you will have a tough time degugging your models, cause they love giving errors. Spare a few minutes analysing your data and understanding it, save hours of debugging, frustration and threats to your sanity. \n\n\n---","metadata":{}},{"cell_type":"markdown","source":"### About External Help \n\nNow like every goood tech person now we will also use ChatGPT, just don't use it mindlessly and understand the code it gives and ensure that you are in a position to understand it and correct it (cause sometimes it can be a big pain in the...). No shame in getting help, we all need help and we have quite the convenient source, but always ensure that your help actually helps you. \n\nYou can also employ human help in the form of friends, CC members, professors or anyone else. Just never be afraid to ask for help when stuck, the help being human or not.","metadata":{}},{"cell_type":"markdown","source":"## On to the Code\n\nBye bye rambling session, let's get down to business","metadata":{}},{"cell_type":"markdown","source":"## Import the necessary Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Let us now load the data**\n\nBased on the environment the paths will change.abs\nThe general flow is :\n - Navigate to the csv\n - Copy the path\n - Paste in the ```read_csv``` function\n\nIn kaggle\n - Navigate to input\n - Go to datasets\n - Click on the drop down arrow next to titanic-dataset\n - Hover over train_and_test2.csv\n - The copy icon appears click that\n\nIn colab (assuming you uploaded the csv\n - Connect to drive\n - Select the file","metadata":{}},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"markdown","source":"We use the pandas function ```read_csv``` this will read our csv file and store it in a Pandas DataFrame.","metadata":{}},{"cell_type":"code","source":"# Load your data here\ndata = pd.read_csv(\"/kaggle/input/titanic-dataset/Titanic-Dataset.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have loaded our data!\nLet us visualize our data\nLet us see whats in the columns","metadata":{}},{"cell_type":"markdown","source":"## Exploring our Data\n\nWe need to fully understand our data, so let us get on to this","metadata":{}},{"cell_type":"markdown","source":"### Display the Data","metadata":{}},{"cell_type":"code","source":"print('Displaying the data') # For those of you unfamiliar with Python this is python's version of printf()\ndata\n# Some fun facts\n# usually doing this doesn't yield results\n# Most of the time you need a print statement\n# But the beauty of a python notebook is, if you give a single variable at the end and run the cell, \n# it displays its contents\n# Provided you haven't given any other variable or print statement after it","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Let's use two Pandas functions to control how much and which part of the data we want to see.","metadata":{}},{"cell_type":"code","source":"data.head(10) # Displays the first 10 values, If you don't specify a value it will return first 5 rows (default)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.tail(12) # Last 15 rows, if number of rows, not specified last 5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We successfully displayed our data!\n\nMoving on, let's analyze more about our data, see the datatypes, how its distributed, if there are any missing values and so on, let us also see all the columns, and check for outliers.\n\nAfter this session try modifying the parameter values passed to ```head``` and ```tail```.","metadata":{}},{"cell_type":"markdown","source":"### Explore the Data","metadata":{}},{"cell_type":"code","source":"data.info() # Gives us basic information about the dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe() # Gives us basic numerical measures for numeric datatypesda","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### See the number of missing values columnwise\n\nWe shall be using ```isnull()``` and ``sum()`` for this.\nWe will be doing something called method chaining - applying two functions sequentially","metadata":{}},{"cell_type":"code","source":"data.isnull().sum() # This is called method chaining using two functions sequentially\n\n# isnull() - gives boolean values for whether a particular value is null or not\n# if null the value is true\n# sum - it sums up the values in a list/array/column\n# now our column is full of booleans - its sums of the True values (True = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Let's explore the columns","metadata":{}},{"cell_type":"code","source":"data.columns # See all the columns in the data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Let us see the values in a particular column","metadata":{}},{"cell_type":"code","source":"data['Column_Name'] # Choose the column you want","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.Column_Name # This also works when we don't have any spaces in the column name","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Little exercise - apply one of above functions to a column","metadata":{}},{"cell_type":"code","source":"data.Column_Name.func() # Replace with a column and function of your choice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in data.columns: # iterating through all the columns\n    print(col, data[col].nunique()) # displaying the number of unique values ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Exercise for you!\n##### See the unique values in the Embarked column \n\n\nHint - the function you should be using is ```unique()```","metadata":{}},{"cell_type":"code","source":"# Write the code here\ndata['Embarked'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization!","metadata":{}},{"cell_type":"markdown","source":"### Import the necessary libraries\n\nMatplotlib and Seaborn are the standard visualization librarires in Python.\nSeaborn is built on top of matplotlib.\nWe will be using these to visualize our data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.hist(data['Age'], edgecolor = 'black', color=(0.2,0.7,1))\nplt.title('Distribution of Ages')\nplt.xlabel('Age')\nplt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.hist(data['Survived'], edgecolor = 'black', color=(0.2,0.7,1))\nplt.title('Distribution of Survival')\nplt.xlabel('Age')\nplt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These are a few basic graphs.\nBut we can make more sophisticated ones - visualizing multiple columns at once.\nThis is where seaborn comes in - its more sophisticated","metadata":{}},{"cell_type":"code","source":"sns.catplot(x='Age', data = data, hue = 'Survived', kind = 'count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This doesn't look that nice..\n\nCan we make it better?\n\nYes!","metadata":{}},{"cell_type":"code","source":"data['Age_Category'] = data.Age # We are creating a column called Age_Category for visualization\n\n# Creating a function - this is just going to categorize people based on age - kinda a binning technique\ndef cats(val):\n    if pd.isna(val):  # Check if the value is NaN\n        return 'NIL' \n    elif val<=12:\n        return 'Child'\n    elif val<=19:\n        return 'Teenager'\n    elif val<=25:\n        return 'Young Adult'\n    elif val<=43:\n        return 'Adult'\n    elif val<= 60:\n        return 'MiddleAged'\n    else:\n        return 'Senior'\n\ndata.Age_Category = data.Age.apply(cats) # apply is used to apply a function\n# The dictionary equivalent is a map, where we map the values based on dictionary keys","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.catplot(x='Age_Category', data = data, hue = 'Survived', kind = 'count', palette='summer')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Much better! Let's also add gender into the picture","metadata":{}},{"cell_type":"code","source":"sns.catplot(x='Age_Category', data = data, hue = 'Survived', kind = 'count', col = 'Sex', palette='spring')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Let us see the survival per age class wise","metadata":{}},{"cell_type":"code","source":"print('Survival by Age Class Wise')\nsns.catplot(x='Age_Category', data = data, hue = 'Survived', kind = 'count', col = 'Pclass', palette='autumn')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Sex and Class","metadata":{}},{"cell_type":"code","source":"print('Survival by class, gender-wise')\nsns.catplot(x='Pclass', data = data, hue = 'Survived', kind = 'count', col = 'Sex', palette = 'husl')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cleaning Data - Removing Missing Values","metadata":{}},{"cell_type":"markdown","source":"Age column has some missing values, let us try to fill them. But before that let us analyze the distribution of Age wrt several parameters.","metadata":{}},{"cell_type":"code","source":"print('Age Distribution Class Wise')\nsns.catplot(x='Age_Category', data = data, kind = 'count', palette='twilight')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Age Distribution Class Wise')\nsns.catplot(x='Age_Category', data = data, kind = 'count', col = 'Pclass', palette='husl')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Age Distribution Class Wise differentiated by gender')\nsns.catplot(x='Age_Category', data = data, kind = 'count', col = 'Pclass', hue = 'Sex', palette='plasma')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see the distribution varies from class to class and also gender wise. Let us group our data class wise and gender wise and then fill in the missing values. Always remember whenever we fill we want to preserve the distribution of the data.","metadata":{}},{"cell_type":"code","source":"data['Age'] = data.groupby(['Sex','Pclass', 'Survived'])['Age'].transform(lambda x : x.fillna(x.mean()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Redefine the categories","metadata":{}},{"cell_type":"code","source":"data.Age_Category = data.Age.apply(cats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Age Distribution Class Wise')\nsns.catplot(x='Age_Category', data = data, kind = 'count', palette='twilight')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Age Distribution Class Wise differentiated by gender')\nsns.catplot(x='Age_Category', data = data, kind = 'count', col = 'Pclass', hue = 'Survived', palette='cool')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Outlier Analysis","metadata":{}},{"cell_type":"markdown","source":"So Outliers are points outside your data's range, hence the name. We don't need to go in depth but they skew your data essentially - making predictions off.\nTo garner an understanding take this example:\nIn a company of 50 employees, 49 employees earn 100rs, but one employee earns 100000rs. Now when we average the salary, it won't be 100 but 6900, which is very off. We want to see what most employees earn, but the outlier affects the average (it is called mean in data science terms). Now this is oversimplified but this definition should suffice for now.","metadata":{}},{"cell_type":"markdown","source":"### Box Plot \nVisualize your outliers\nThe box portion of the graph shows the range of the median values. The part between the whiskers (two horizontal black lines) shows the true distribution. Points outside the whiskers are outliers.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=data['Age'], color = (0.8, 0.6,1)) # The points outside the whiskers are outliers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Remove the Outliers","metadata":{}},{"cell_type":"markdown","source":"We are going to be removing the Outliers using IQR - Inter Quartile Range.\nYou can consider it this way - every data follows a distribution, we are cutting the endmost portions cause that is where the outliers lie. Simple everyday example - cutting splitends - except here we are cutting on both ends.","metadata":{}},{"cell_type":"code","source":"def iqr(data):\n    q1 = data.quantile(0.25)\n    q3 = data.quantile(0.75)\n\n    iqr = q3 - q1\n\n    return iqr, q1, q3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"iqr, q1, q3 = iqr(data['Age'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"upper_limit = q3 + 1.5*iqr\nlower_limit = q1 - 1.5*iqr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_cleaned = data[(data['Age'] <= upper_limit) & (data['Age'] >= lower_limit)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### See the cleaned data","metadata":{}},{"cell_type":"code","source":"data_cleaned.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Box plot to see how the outliers were removed","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=data_cleaned['Age'], color = (0.7, 0.6,1)) # The points outside the whiskers are outliers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cleaning Part 2 - Removing Unnecessary Columns","metadata":{}},{"cell_type":"markdown","source":"When it comes to world of Data Science we encounter something called \"Resource Bottlenecks\". This exhibits itself in the form of insufficient computational power. Now size affects this a lot. You want to train something faster - make it smaller. But less data = equals a not very robust model. This is why we keep the data we need and normalize it. We get rid of unwanted data. The normalizing part is for another session. The deleting part - we are doing it now.","metadata":{}},{"cell_type":"markdown","source":"In our case the unnecessary columns are Name, Ticket, Cabin, Embarked. We don't gain much information from them","metadata":{}},{"cell_type":"code","source":"data_cleaned.drop(['Ticket', 'Cabin', 'Embarked', 'Name'], axis = 1, inplace = True) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"axis 1 means columns\n\ninplace true - this will apply the function on the dataframe rather than on a copy, be careful when using this, if you make a mistake its hard to correct, you will have to run all cells again - the purpose of a notebook is avoiding that","metadata":{}},{"cell_type":"code","source":"data_cleaned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"On our level we can ignore embarked, but for a higher level analysis keeping it might be useful as there might be a small relation with other columns. If a particular area has more rich people then if a person is from that place they have a higher chance of surviving but, currently we are more focused on lower order relationships. This is a fun thing to note tho. But we already have a better column Pclass which gives us more insights into who survived","metadata":{}},{"cell_type":"markdown","source":"### Exercise for you!\nDrop the Fare column - we already know the class details, so no need for feature engineering from the ticket Fare.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Don't stop here. Experiment more with this dataset. We shall be seeing some other pandas functions and methods. ","metadata":{}},{"cell_type":"markdown","source":"# Structured Workflow with some more Functions","metadata":{}},{"cell_type":"markdown","source":"Here is a general flow and some more functions that you can use as a reference.","metadata":{}},{"cell_type":"markdown","source":"## Step 0: Imports and Reading Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nplt.style.use('ggplot')\npd.set_option('display.max_columns', 200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/rollercoaster-database/coaster_db.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Data Understanding\n- Dataframe `shape`\n- `head` and `tail`\n- `dtypes`\n- `describe`\n","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Data Preperation\n- Dropping irrelevant columns and rows\n- Identifying duplicated columns\n- Renaming Columns\n- Feature Creation","metadata":{}},{"cell_type":"code","source":"# Example of dropping columns\n# df.drop(['Opening date'], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[['coaster_name',\n    # 'Length', 'Speed',\n    'Location', 'Status',\n    # 'Opening date',\n    #   'Type',\n    'Manufacturer',\n#     'Height restriction', 'Model', 'Height',\n#        'Inversions', 'Lift/launch system', 'Cost', 'Trains', 'Park section',\n#        'Duration', 'Capacity', 'G-force', 'Designer', 'Max vertical angle',\n#        'Drop', 'Soft opening date', 'Fast Lane available', 'Replaced',\n#        'Track layout', 'Fastrack available', 'Soft opening date.1',\n#        'Closing date',\n#     'Opened', \n    # 'Replaced by', 'Website',\n#        'Flash Pass Available', 'Must transfer from wheelchair', 'Theme',\n#        'Single rider line available', 'Restraint Style',\n#        'Flash Pass available', 'Acceleration', 'Restraints', 'Name',\n       'year_introduced',\n        'latitude', 'longitude',\n    'Type_Main',\n       'opening_date_clean',\n    #'speed1', 'speed2', 'speed1_value', 'speed1_unit',\n       'speed_mph', \n    #'height_value', 'height_unit',\n    'height_ft',\n       'Inversions_clean', 'Gforce_clean']].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['opening_date_clean'] = pd.to_datetime(df['opening_date_clean'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rename our columns\ndf = df.rename(columns={'coaster_name':'Coaster_Name',\n                   'year_introduced':'Year_Introduced',\n                   'opening_date_clean':'Opening_Date',\n                   'speed_mph':'Speed_mph',\n                   'height_ft':'Height_ft',\n                   'Inversions_clean':'Inversions',\n                   'Gforce_clean':'Gforce'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[df.duplicated()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for duplicate coaster name\ndf.loc[df.duplicated(subset=['Coaster_Name'])].head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking an example duplicate\ndf.query('Coaster_Name == \"Crystal Beach Cyclone\"')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.loc[~df.duplicated(subset=['Coaster_Name','Location','Opening_Date'])] \\\n    .reset_index(drop=True).copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Feature Understanding\n(Univariate analysis)\n\n- Plotting Feature Distributions\n    - Histogram\n    - KDE\n    - Boxplot","metadata":{}},{"cell_type":"code","source":"df['Year_Introduced'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = df['Year_Introduced'].value_counts() \\\n    .head(10) \\\n    .plot(kind='bar', title='Top 10 Years Coasters Introduced')\nax.set_xlabel('Year Introduced')\nax.set_ylabel('Count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = df['Speed_mph'].plot(kind='hist',\n                          bins=20,\n                          title='Coaster Speed (mph)')\nax.set_xlabel('Speed (mph)')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = df['Speed_mph'].plot(kind='kde',\n                          title='Coaster Speed (mph)')\nax.set_xlabel('Speed (mph)')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Type_Main'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Feature Relationships\n- Scatterplot\n- Heatmap Correlation\n- Pairplot\n- Groupby comparisons","metadata":{}},{"cell_type":"code","source":"df.plot(kind='scatter',\n        x='Speed_mph',\n        y='Height_ft',\n        title='Coaster Speed vs. Height')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = sns.scatterplot(x='Speed_mph',\n                y='Height_ft',\n                hue='Year_Introduced',\n                data=df)\nax.set_title('Coaster Speed vs. Height')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.pairplot(df,\n             vars=['Year_Introduced','Speed_mph',\n                   'Height_ft','Inversions','Gforce'],\n            hue='Type_Main')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_corr = df[['Year_Introduced','Speed_mph',\n    'Height_ft','Inversions','Gforce']].dropna().corr()\ndf_corr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.heatmap(df_corr, annot=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Ask a Question about the data\n- Try to answer a question you have about the data using a plot or statistic.\n\nWhat are the locations with the fastest roller coasters (minimum of 10)?","metadata":{}},{"cell_type":"code","source":"ax = df.query('Location != \"Other\"') \\\n    .groupby('Location')['Speed_mph'] \\\n    .agg(['mean','count']) \\\n    .query('count >= 10') \\\n    .sort_values('mean')['mean'] \\\n    .plot(kind='barh', figsize=(12, 5), title='Average Coast Speed by Location')\nax.set_xlabel('Average Coaster Speed')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Miscellaneous","metadata":{}},{"cell_type":"markdown","source":"Here are some things you need to know about Pandas and in the end we will provide you with functions you can look up. These are more higher order functions and hence they have been added at the last like an afterthought (which they very much are).","metadata":{}},{"cell_type":"markdown","source":"## Creating your own DataFrame","metadata":{}},{"cell_type":"code","source":"# Dictionary with student details\nstudent_details = {'Names': ['A', 'B', 'C', 'D'],\n                  'Roll_no':[1,2,3,4]}\n\nstudent_data = pd.DataFrame(student_details) #using DataFrame function to create your own dataframe\n\nstudent_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# From lists \nStudents = [[1,'A'], [2,'B'], [3,'C']]\n\nstudent_data_from_list = pd.DataFrame(Students, columns = ['Roll_no', 'Names'])\nstudent_data_from_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can create a DataFrame from Pandas Series too.\nIn a DataFrame each column is stored as a Pandas Series.","metadata":{}},{"cell_type":"code","source":"type(student_data_from_list['Names'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Names = ['A', 'B', 'C', 'D']\nRoll_no= [1,2,3,4]\n\nNames_series = pd.Series(Names)\nRoll_no_series = pd.Series(Roll_no)\n\nStud_data = pd.DataFrame({'Names':Names_series, 'Roll_no':Roll_no_series})\n\n# this is a dictionary only but rather than a dictionary of lists, its a dictionary of series\n\nStud_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can try various permutations and combinations. What method to use is based on your convenience. \nA benefit of using Series is, even if the size doesn't match, if you supply index postions, the dataframe will be created with NaN values wherever data isn't available. The other two methods will give you errors if you try the same.","metadata":{"execution":{"iopub.status.busy":"2025-01-17T12:16:25.275290Z","iopub.execute_input":"2025-01-17T12:16:25.275682Z","iopub.status.idle":"2025-01-17T12:16:25.284057Z","shell.execute_reply.started":"2025-01-17T12:16:25.275649Z","shell.execute_reply":"2025-01-17T12:16:25.283184Z"}}},{"cell_type":"markdown","source":"## Some more Functions\n\n| **Function** | **Purpose** | **Key Feature** |\n|--------------|-------------|-----------------|\n| `pd.merge`   | Merge two DataFrames based on common columns or indexes (joins). | Like SQL JOIN. |\n| `pd.concat`  | Combine DataFrames along rows or columns. | Keeps all data, can stack DataFrames. |\n| `pd.pivot`   | Reshape data by turning a column’s unique values into columns. | Used for restructuring data. |\n","metadata":{}},{"cell_type":"code","source":"# pd.merge example\ndf1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\ndf2 = pd.DataFrame({'ID': [1, 2, 4], 'Age': [25, 30, 40]})\n\n# Merge on the 'ID' column\nmerged_df = pd.merge(df1, df2, on='ID', how='inner')  # Inner join\nprint(\"Merged DataFrame:\")\nprint(merged_df)\nprint()\n\n# pd.concat example (rows)\ndf3 = pd.DataFrame({'ID': [1, 2], 'Name': ['Alice', 'Bob']})\ndf4 = pd.DataFrame({'ID': [3, 4], 'Name': ['Charlie', 'David']})\n\n# Concatenate along rows\nconcat_rows_df = pd.concat([df3, df4], axis=0)\nprint(\"Concatenated DataFrame (Rows):\")\nprint(concat_rows_df)\nprint()\n\n# pd.concat example (columns)\ndf5 = pd.DataFrame({'ID': [1, 2]})\ndf6 = pd.DataFrame({'Name': ['Alice', 'Bob']})\n\n# Concatenate along columns\nconcat_columns_df = pd.concat([df5, df6], axis=1)\nprint(\"Concatenated DataFrame (Columns):\")\nprint(concat_columns_df)\nprint()\n\n# pd.pivot example\ndata = pd.DataFrame({\n    'Date': ['2025-01-01', '2025-01-01', '2025-01-02', '2025-01-02'],\n    'City': ['New York', 'Los Angeles', 'New York', 'Los Angeles'],\n    'Temperature': [32, 75, 30, 72]\n})\n\n# Pivot the DataFrame\npivot_df = data.pivot(index='Date', columns='City', values='Temperature')\nprint(\"Pivoted DataFrame:\")\nprint(pivot_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving your DataFrame\n\nLast but not least, saving your DataFrame. After all the cleaning and preprocessing saving your csv is crucial to move onto other tasks (Encoding, Model Training etc.) The most popular format for saving is as a csv which we will be doing. We use the `to_csv()` function for this.","metadata":{}},{"cell_type":"code","source":"pivot_df.to_csv('Saved.csv') # to_csv - converts a DataFrame to a csv file","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Don't stop here, keep exploring, try out other functions, keep learning. Google, ChatGPT and Documentation exists, make the most of it.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.pinimg.com/originals/8c/40/05/8c4005377742272315e792545a9c93df.gif\" alt=\"Cute Gif\" width=\"600\">\n","metadata":{}}]}